# Skip-gram-Language-Model-Word2Vec-
This project implements the word2vec skip-gram  architecture model from scratch. It computes the  embedding vector for the different text/word in the Brown Corpus

A skip-gram model predicts the context words surrounding a given (current) word. It tries to learn the probability distribution of the surrounding words given a current/target word. Skip-gram models are used in generating embedding vectors of words in NLP tasks. Embedding vectors are dense, low-dimensional representations of categorical variables (e.g. words NLP tasks) in continuous vector space. 

Simply it means converting categorical variables into numerical or contonous data points that can then be processed and understood by a computer. Skipgram helps us generate these embedding representations.   

The embedding of various words in the Brown Corpus is computed in this project. Download the Brown Corpous txt file here. 
